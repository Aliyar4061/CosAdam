# CosAdam

-Cosine-Similarity-Guided-Adaptive-Moment-Estimation-for-Deep-Learning-Optimization

Optimization in deep neural networks remains a critical challenge, directly influencing training efficiency, convergence behavior, and generalization. First-order adaptive methods, such as Adam, are widely employed due to their computational scalability and parameter-wise learning rate adaptation. However, Adam and its variants often suffer from convergence instability in non-convex landscapes, overfitting due to aggressive adaptation, and sensitivity to hyperparameter tuning. While prior works have attempted to mitigate these issues via techniques like decoupled weight decay, momentum scheduling, or bias correction, these approaches primarily refine existing mechanisms without incorporating novel directional insights. In this work, we identify \emph{directional instability} the inability of optimizers to distinguish constructive gradient alignment from detrimental oscillations as a fundamental limitation of current adaptive methods. To address this, we propose CosAdam, a novel optimizer that introduces \emph{directional consistency monitoring} through an exponential moving average of cosine similarity between successive gradients.  At the core of CosAdam is the Directional Consistency Factor (DCF): (\( s_t = \alpha s_{t-1} + (1-\alpha) \frac{g_t \cdot g_{t-1}}{\|g_t\| \|g_{t-1}\|} \))  which adaptively modulates the step size amplifying updates during stable,  which adaptively modulates the step size amplifying updates when gradients are aligned ($s_t \rightarrow 1$) and suppressing them during noise-induced divergence ($s_t \rightarrow -1$). CosAdam maintains $\mathcal{O}(N)$ ($N$ is the total number of model parameters) time and space complexity, with minimal overhead, and integrates seamlessly with AdamW-style decoupled weight decay. Extensive evaluations on CIFAR-10, MNIST, noisy MNIST, and SST-2 benchmarks demonstrate its superiority: achieving 89.09\% accuracy on CIFAR-10 (vs. 81.15\% for Adam and 88.13\% for AdamW), 82.91\% on SST-2, and 96.64\% with a 96.61\% F1-score on noisy MNIST. Empirical and theoretical analyses confirm improved convergence and robustness in noisy and non-stationary regimes. CosAdam emerges as a robust, drop-in replacement for existing optimizers, particularly effective in scenarios demanding directional stability.
